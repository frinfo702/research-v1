# -*- coding: utf-8 -*-
"""sensing2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iNXocslOxHgzMQvkPy1-YBZymrXUOKVJ

## センシング工学第11回 最小二乗法(4)

前回までで最尤推定、最小二乗法による直線当てはめ、共分散・相関を考慮した誤差伝播則について学んだ。今回はpython/numpy/mapplotlibによるデータ処理の演習を行う。

### numpyの基礎
"""

# matplot.pyplotでグラフやプロットの描画
import matplotlib.pyplot as plt
# numpyで数値や行列、基本的な統計処理をする
import numpy as np

# np.arrayの練習
# これはpythonのlist型
a = [2,3,4]
print(a)
print(type(a))

# list型を「足す」とどうなるか？
[2,3,4]+[0,-2,4]

# numpyのarray型
b = np.array(a)
print(b)
print(type(b))

# numpy array型の足し算
c = np.array([0,-2,4])
print(b,c)
b + c # 要素ごとに足し算できる

b*c # 要素ごとに掛け算もできる。

print(b)
b**2,np.sqrt(b) # 要素ごとの累乗,平方根

print(b,c)
np.dot(b,c) # ベクトルだと思って内積を取る

M = np.array([[1,2,3],[3,2,1]]) # 2x3行列
M

v = np.array([2,-1,3])
print(M,v)
M@v # 行列の積

M*v # *だと要素ごとの積を取ってしまう

# ベクトルの作成
vec1 = np.array([1, 2, 3, 4, 5])
print("基本的なベクトル:", vec1)
print("型:", type(vec1))
print("形状:", vec1.shape)
print()

#ベクトルのスライシング
print("最初の3つの要素:", vec1[:3])
print("最後の3つの要素:", vec1[-3:])
#print("2番目から4番目まで:", vec1[??])
#print("1つ飛ばしで取得:", vec1[??])
print()

#条件に基づく要素の取得
print("3より大きい要素:", vec1[vec1 > 3])
print()

#行列の作成
matrix1 = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])
print(matrix1)

#２次元のスライシング
print("左上２x２")
print(matrix1[0:2,0:2])
print()
print("下２x３")
print(matrix1[0:2,0:3])



"""### データプロットと回帰"""

# numpy.ndarray.astype()は型変換
x = np.array([1,2,3,4,5]).astype(float)
y = np.array([4,2,1,2,0]).astype(float)
print(x,y)

print(len(x),len(y)) # len(x)はxの第1次元の要素数
print("x=%s y=%s"%(x,y))

"""matplotlib.pyplot.scatter関数をつかって散布図を作ってみよう。
matplotlib.pyplot.scatter 関数で使用できる marker 引数のオプションは多様で、さまざまな形状のマーカーを指定することができます。以下は、よく使用されるマーカーの種類の一部です：

    'o'：円
    '.'：点
    ','：ピクセル
    'x'：バツ
    '+'：プラス
    '_'：水平線
    '|'：垂直線
    's'：正方形
    'd'：菱形
    'D'：大きな菱形
    'p'：五角形
    'h'：六角形1
    'H'：六角形2
    '*'：星
    '<'：三角形左
    '>'：三角形右
    '^'：三角形上
    'v'：三角形下
    None, ' ' または ''：マーカーなし
"""

# matplotlib.pyplotのscatter関数を使って散布図プロットを作ってみる。
plt.scatter(x,y,marker='.',color="blue")
plt.xlabel('x') # 横軸のキャプション
plt.ylabel('y') # 縦軸のキャプション
plt.show() # 表示（jupyter noteboookやgoogle colabならセルに表示される）

"""## 最小二乗法による直線当てはめ y=A+Bx
正規方程式
\begin{eqnarray}
AN+B\sum x &=& \sum y \\
A\sum x + B\sum x^2 &=& \sum xy
\end{eqnarray}
これを解くと
\begin{eqnarray}
A &=& \frac{(\sum x^2)(\sum y)-(\sum x)(\sum xy)}{\Delta}, \,\,\,
B &=& \frac{(\sum x)(\sum y)-N(\sum xy)}{\Delta}\\
\Delta &=& N\sum x^2 - (\sum x)^2
\end{eqnarray}
AとBの推定誤差の分散は
\begin{eqnarray}
\sigma_A &=& \sigma_y\sqrt{\frac{\sum x^2}{\Delta}}, \,\,\,
\sigma_B = \sigma_y\sqrt{\frac{N}{\Delta}} \\
\sigma_y &=& \sqrt{\frac{1}{N-2}\sum (y-A-Bx)^2}
\end{eqnarray}
"""

# 最小二乗法による直線当てはめ y=A+Bx
def fit_line(x,y):
  if len(x) != len(y):
      print("xとyのデータ数が違うよ！")
      exit
  print('x:',x)
  print('y:',y)
  N = len(x)
  sum_x = np.sum(x)
  sum_y = np.sum(y)
  sum_xx = np.sum(x**2)
  sum_xy = np.sum(x*y)
  Delta = N*sum_xx-(sum_x)**2
  print(f"N={N}\nsum(x)={sum_x}\nsum(y)={sum_y}\nsum(x^2)={sum_xx}\nsum(xy)={sum_xy}\nDelta={Delta}")

  A = (sum_xx*sum_y-sum_x*sum_xy)/Delta
  B = (N*sum_xy-sum_x*sum_y)/Delta
  sigma_y = np.sqrt(np.sum((y-A-B*x)**2)/(N-2))
  sigma_A = sigma_y*np.sqrt(sum_xx/Delta)
  sigma_B = sigma_y*np.sqrt(N/Delta)
  return A,B,sigma_A,sigma_B,sigma_y

A,B,sA,sB,sy = fit_line(x,y)
print(f"A={A}\nB={B}\nsigmaA={sA}\nsigmaB={sB}\nsigmay={sy}")

#(x1,y1)から(x2,y2)へ直線を描く
#plt.plot([x1,x2],[y1,y2]) #最初にxの区間、次にyの区間であることに注意
plt.plot([0,10],[1,5],color="red",linestyle="-")
plt.plot([-1,7],[8,-3],color="green",linestyle="--",linewidth=3)
plt.show()

# matplotlib.pyplot.plot 関数で使用できる linestyle 引数のオプションは以下の通りです：
#    '-' または 'solid' : 実線（デフォルト）
#    '--' または 'dashed' : 破線
#    '-.' または 'dashdot' : 点線と破線の組み合わせ
#    ':' または 'dotted' : 点線
#    '' または ' ' または None : 線なし（マーカーのみ表示）
#
# 線幅はlinewidthに指定する

plt.scatter(x,y,marker='.')
A,B,sA,sB,sy = fit_line(x,y)
print("A={A} B={B}, sigmaA={sA} sigmaB={sB} sigmay={sy}")

def y_est(x,A,B):
    return A+B*x

xmin = np.min(x) # 最小値
xmax = np.max(x) # 最大値
plt.plot([xmin,xmax],[y_est(xmin,A,B),y_est(xmax,A,B)],color="red")
plt.show()

"""### ライブラリモジュールを使った最小二乗法"""

# np.polyfit(x,y,dim) # y=f(x;p)(fは次数dimの回帰多項式、pは回帰多項式の係数)
# 返り値はnp.ndarray(1次元)。高い次数の係数から並ぶのでpred[0]が1次、pred[1]が0次の係数
pred = np.polyfit(x,y,1)
print(pred)
plt.scatter(x,y,marker='.')
xmin = np.min(x)
xmax = np.max(x)
A=pred[1]
B=pred[0]
plt.plot([xmin,xmax],[y_est(xmin,A,B),y_est(xmax,A,B)],color="red")
plt.plot([xmin,xmax],[y_est(xmin,A,B),y_est(xmax,pred[1],pred[0])],color="green")
plt.show()

"""２次式以上のプロットにはnp.polyval()を使ってちょっと工夫するとよい。"""

# np.polyfit(x,y,dim) # y=f(x;p)(fは次数dimの回帰多項式、pは回帰多項式の係数)
# 返り値はnp.ndarray(1次元)。高い次数の係数から並ぶ
dim = 2  # 次数を変更可能
pred = np.polyfit(x, y, dim)
print(pred)

plt.scatter(x, y, marker='.')

# 曲線描画用に細かいx座標を生成
xmin = np.min(x)
xmax = np.max(x)
x_line = np.linspace(xmin, xmax, 100)

# np.polyvalで多項式を評価（任意の次数に対応）
y_line = np.polyval(pred, x_line)

plt.plot(x_line, y_line, color="red")
plt.show()

# 2次式以上でフィッティングしてみる
def polynomial_fit(x,y,maxdim=4,xlim=[],ylim=[]):
  cmap = plt.get_cmap("tab10") #色のindexを並べたcolormapを取得
  pred = []
  for dim in range(maxdim+1):
    #dim次元の多項式で最小二乗法。
    #cov=Trueの時は(推定値、共分散行列)のタプルが帰る
    if dim<len(x)-1 :
      cov = True
    else : # dimがデータ数-1の場合は共分散が計算できない
      cov = False
    pred.append(np.polyfit(x,y,dim,cov=cov))
  #print(f"{pred}")

  plt.scatter(x,y,marker='.')
  if xlim == []:
    xmin = np.min(x)
    xmax = np.max(x)
  else:
    xmin = xlim[0]
    xmax = xlim[1]

  xs = np.linspace(xmin, xmax, 100)
  plt.xlabel("x")
  plt.ylabel('y')
  plt.ylim(*ylim) # "*"演算子はリストやタプルを受けて中身を展開して関数の引数に渡す時に使う
  for dim in range(maxdim):
    # np.polyval(p,x): 多項式の係数がpであるときにxに対する式の値を返す。
    if dim <len(x)-1 :
      p = pred[dim][0]
    else :
      p = pred[dim]
    plt.plot(xs,np.polyval(p,xs),label='%d-dim'%dim,color=cmap(dim))

  plt.legend()
  return pred

pred = polynomial_fit(x,y)

pred[0] #0次式でのフィッティングにおける推定値（平均）と推定誤差の分散

pred[1] #1次式でのフィッティングにおける推定値（A=p[1],B=p[0]）と推定誤差の共分散

pred[2] #2次式でのフィッティングにおける推定値（平均）と誤差の分散

"""当てはめる多項式の次数が増えるに従って、返ってくる多項式のパラメータと共分散行列の次数はどうなっていますか？

### もっと多くのデータサンプルについて回帰してみる練習

ランダムノイズが加わったデータを作って、polyfitを使って多項式を回帰してみてください。
"""

# 真のデータを生成するモデル
def model_func(x):
    #return x**3-4*x**2-4
    return 5*x-10

# モデル曲線の描画
xtrue = np.linspace(-5,10,100)
ytrue = model_func(xtrue)
plt.plot(xtrue,ytrue,label="model",color="black")

# 真のデータ点の生成（スパースにとる）
x2 = np.linspace(-2,5,20)
y2 = model_func(x2)
# ノイズの乗ったデータ点の生成
ynoise = np.random.normal(0,8,len(x2)) # 平均0、分散8の正規分布からの独立サンプル
y2_noise = y2 + ynoise
plt.ylim(-40,40)
plt.xlabel('x')
plt.ylabel('y')
#plt.plot(x2,y2_noise,'.',color="blue")
plt.scatter(x2,y2_noise,marker='.')
plt.legend()
plt.show()

pred = polynomial_fit(x2,y2_noise,maxdim=4,xlim=[-5,10],ylim=[-40,40])

"""フィットする多項式の次数(maxdim)を増やしていくと推定した曲線はどうなるか確認しよう。
データ点数が20点なら19次まで形式的には当てはめることができる。その結果は妥当だろうか？

### 当てはめた直線の誤差を可視化しよう

講義では$y=A+Bx$の当てはめにおいて、Aの推定値の推定誤差の分散$\sigma_A^2,Bに対する\sigma_B^2$のみ導出したが、上の計算でdim=1の場合のpolyfitの返り値は2x2の対称行列になっている。対角成分はそれぞれ$\sigma_A^2$と$\sigma_B^2$に対応するがそうでない成分はなんだろうか？実はこれは$A$の推定誤差と$B$の推定誤差の「共分散」$\sigma_{AB}$である。

すなわち、$A$と$B$の推定値は必ずしも無相関ではない。$\sigma_{AB}\neq 0$の場合はAの誤差とBの誤差には関係が出てくる。この共分散の導出方法は授業ではまだ述べていなかった。次の補足資料を読むとある程度理解できるだろう。
https://ct.ritsumei.ac.jp/ct/page_8536308c7072625_2420105785_2420105784/%E3%82%BB%E3%83%B3%E3%82%B7%E3%83%B3%E3%82%AF%E3%82%99%E5%B7%A5%E5%AD%A6%E7%B7%9A%E5%BD%A2%E5%BD%93%E3%81%A6%E3%81%AF%E3%82%81.pdf?view=full

AとBが$\sigma_A^2$,$\sigma_B^2$,$\sigma_{AB}$の分散・共分散をもつ誤差を含む場合、$y=A+Bx$による$y$の推定値$\hat{y}$の誤差の分散($\sigma^2_{\hat{y}}$)を<u>**誤差が独立ではない場合の誤差伝播則**</u>を用いて計算すると(xの誤差は独立変数なので0とする)、
\begin{eqnarray}
\sigma^2_{\hat{y}} &=& \left(\frac{\partial y}{\partial A}\right)^2\sigma_A^2+\left(\frac{\partial y}{\partial B}\right)^2\sigma_B^2+2\left(\frac{\partial y}{\partial A}\right)\left(\frac{\partial y}{\partial B}\right)\sigma_{AB} \\
&=& \sigma^2_Bx^2+2\sigma_{AB}x+\sigma^2_A\\
\,
\end{eqnarray}
これは$x$の二次関数である。$y$の真の値は誤差の標準偏差$\sigma_{\hat{y}}$を考慮して$\hat{y}\pm2\sigma_{\hat{y}}$の間95\%の確率で入ってくると考えることは妥当である(誤差が正規分布するという仮定の下では）。

そこで、この値をデータに対してplotしてみることにしよう。$x$ごとの$y\pm2\sigma_{\hat{y}}$を同時に描いてみればよい。
"""

# A,Bの分散、共分散を取り出す
p_linear = pred[1][0] # 1次式での回帰パラメータ
sA2 = pred[1][1][1,1] # sigma^2_A
sB2 = pred[1][1][0,0] # sigma^2_B
sAB = pred[1][1][0,1] # sigma_AB, pred[1][1][1,0]でもよい（共分散行列は対称なので）
sA2,sB2,sAB

# yの推定誤差を考慮して推定直線の上下に95%信頼区間を描画してみる
sigma2_yhat = xtrue**2*sB2+2*xtrue*sAB+sA2 #sigmaはscalarだがxsは1次元のnp.array
s_yhat = np.sqrt(sigma2_yhat)
plt.plot(xtrue,ytrue,label='model',color="black")

yhat = np.polyval(p_linear,xtrue)
plt.plot(xtrue,yhat,label='y_hat',color='green')
plt.plot(xtrue,yhat+2*s_yhat,label='+2sigma',color="red")
plt.plot(xtrue,yhat-2*s_yhat,label='-2sigma',color="blue")
plt.ylim(-40,40)
plt.xlabel('x')
plt.ylabel('y')
plt.legend()

# データ
plt.scatter(x2,y2_noise,marker='.')
plt.show()

"""上の黒の直線がデータを生成したモデルである（つまり真の直線）。<font color="#008800">緑ライン</font>は線形回帰によって当てはめた直線（最小二乗法によって最尤推定されたもの）。<font color="#ff0000">赤</font>と<font color="#0000ff">青</font>のラインは、得られたデータから推定された、真の$y$が約95\%の確率で入る区間の<font color="#ff0000">上限</font>と<font color="#0000ff">下限</font>（$\hat{y}\pm2\sigma_{\hat{y}}$）である。

これをみると、$\hat{y}$の誤差（つまり赤と青のラインのy方向への間隔）が最小なのはデータ点の$x$座標の平均の位置であることがわかる。その時推定された回帰直線はデータ点の$y$の平均$(x,y)$の重心を通ることもわかる（簡単に証明できるが各自の課題とする）。重心の位置における$\hat{y}$の誤差の大きさは直線の切片パラメータ（つまり$A$）の誤差と等しい。

さらにわかることはデータ点の$x$の平均から離れるに従って$\hat{y}$の誤差は大きくなることがわかる。これは直線の傾き$B$が持つ誤差によって直線が回転する方向に不確かだということを意味している。回帰直線は必ずデータの重心を通るからここを中心に回転することになるわけで、重心から離れるに従って直線上の点（つまり$x$に対応する$y$の推定値）が大きく動くことになる。これは「外挿」が「内挿」よりも不確かになる、という我々の経験的事実と合致している。

Pythonではnumpyの他にさまざまな統計解析モジュールパッケージが公開されており、機械学習用のパッケージに含まれていることもある。また描画に使ったpyplotはmatplotlibという可視化パッケージの一部であり、この他にも3次元プロットやアニメーションなど数多くの機能を持っている。ぜひ自分でも試してみてほしい。
"""

